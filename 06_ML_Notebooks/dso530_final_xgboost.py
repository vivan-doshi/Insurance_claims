# -*- coding: utf-8 -*-
"""DSO530_FINAL_XGBOOST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nUVY9axPzmwChRsIGeBtwAZhNfblhduk
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import linear_model
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LassoCV, RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import roc_auc_score

# prompt: connect google colab

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/ML Project/')

# Confirm that the files are accessible
os.listdir()

test_df = pd.read_csv('cleaned_test.csv')
df = pd.read_csv('cleaned_data.csv')

# prompt: calculate all nan values

# Calculate the number of NaN values in each column of the DataFrame 'df'
nan_counts = df.isna().sum()

# Print the NaN counts for each column
print(nan_counts)

# Calculate the total number of NaN values in the DataFrame 'df'
total_nan_count = nan_counts.sum()

# Print the total number of NaN values
print(f"\nTotal number of NaN values in the DataFrame: {total_nan_count}")

# prompt: add 0 for Loss_Cost and Historically_Adjusted_Loss_Cost where nan and 'N' for Energy_Source where nan

import pandas as pd
import numpy as np
# ... (rest of your imports and code)

# ... (your existing code)

# Fill NaN values in 'Loss_Cost' and 'Historically_Adjusted_Loss_Cost' with 0
df['Loss_Cost'].fillna(0, inplace=True)
df['Historically_Adjusted_Loss_Cost'].fillna(0, inplace=True)

# Fill NaN values in 'Energy_Source' with 'N'
df['Energy_Source'].fillna('N', inplace=True)

# Calculate the number of NaN values in each column of the DataFrame 'df'
nan_counts = df.isna().sum()

# Print the NaN counts for each column
print(nan_counts)

# Calculate the total number of NaN values in the DataFrame 'df'
total_nan_count = nan_counts.sum()

# Print the total number of NaN values
print(f"\nTotal number of NaN values in the DataFrame: {total_nan_count}")

# STEP 4: Drop leakage and non-predictive columns
drop_cols = [
    'Unnamed: 0', 'ID',
    'Start_Date_Contract', 'Date_Last_Renewal', 'Date_Next_Renewal',
    'Date_Of_Birth', 'Date_Of_DL_Issuance',
    'Loss_Cost', 'Historically_Adjusted_Loss_Cost', 'Claim_Status',
    'Total_Cost_Claims_Current_Yr', 'Total_Number_Claims_Current_Yr',
    'Total_Number_Claims_Entire_Duration', 'Ratio_Claims_Total_Duration_Force'
]

X_raw = df.drop(columns=drop_cols)
y_lc = df['Loss_Cost']
y_halc = df['Historically_Adjusted_Loss_Cost']

X_encoded = pd.get_dummies(X_raw, drop_first=True)

top_features = [
    'Premium_Amt_Current_Yr', 'Customer_Loyalty', 'Power_Wt_Ratio', 'Vehicle_Power_HP',
    'Vehicle_Wt_Kg', 'Ratio_Premium_Car_Value', 'Car_Age', 'Cylinder_Capacity',
    'Car_Age_Cat_Recent', 'Car_Age_Cat_Standard', 'Max_Product_Simultaneous_Held',
    'Years_Driving', 'Non_Continuation_Insurance_Flag', 'Motorbikes_Vans_Cars_Agricultural',
    'Policies_Terminated_Non_Payment'
]
X_lc = X_encoded[top_features]
X_halc = X_encoded[top_features]

!pip install xgboost optuna scikit-learn pandas numpy

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor, XGBClassifier
import optuna

def train_zit_xgb(X, y, n_trials=50, random_state=42):
    """
    Train a zero‑inflated Tweedie model (classifier × reg:tweedie severity)
    with Optuna tuning over learning‑rate and Tweedie power.

    Parameters
    ----------
    X : pandas DataFrame or 2‑D array
    y : pandas Series / 1‑D array (lc or halc)
    n_trials : Optuna trials to run
    random_state : for reproducibility

    Returns
    -------
    model_dict : {
        'clf'   : fitted XGBClassifier,
        'reg'   : fitted XGBRegressor,
        'study' : Optuna study object,
        'predict' : callable -> array of expected losses
    }
    """
    # ──────────────────────────────────────────────────────────
    # 0.  Clean rows with missing target
    mask = ~pd.isna(y)
    X_clean, y_clean = X[mask], y[mask]

    # ──────────────────────────────────────────────────────────
    # 1.  Stage‑1 classifier (zero vs non‑zero)
    y_is_pos = (y_clean > 0).astype(int)
    clf = XGBClassifier(
        objective="binary:logistic",
        n_estimators=400,
        learning_rate=0.05,
        max_depth=4,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=(len(y_clean) - y_is_pos.sum()) / y_is_pos.sum(),  # balance 0/1
        random_state=random_state,
        n_jobs=-1
    )
    clf.fit(X_clean, y_is_pos)

    # ──────────────────────────────────────────────────────────
    # 2.  Prepare data for severity model (only positive rows)
    X_sev = X_clean[y_is_pos == 1]
    y_sev = y_clean[y_is_pos == 1]

    # ──────────────────────────────────────────────────────────
    # 3.  Hyper‑parameter tuning with Optuna
    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)

    def objective(trial):
        p   = trial.suggest_float('p', 1.05, 1.95)
        eta = trial.suggest_float('eta', 0.01, 0.2, log=True)
        depth = trial.suggest_int('max_depth', 3, 8)
        subs  = trial.suggest_float('subsample', 0.6, 1.0)
        col   = trial.suggest_float('colsample_bytree', 0.6, 1.0)
        n_est = trial.suggest_int('n_estimators', 300, 2000)

        reg = XGBRegressor(
            objective="reg:tweedie",
            tweedie_variance_power=p,
            learning_rate=eta,
            max_depth=depth,
            subsample=subs,
            colsample_bytree=col,
            n_estimators=n_est,
            random_state=random_state,
            eval_metric="rmse",
            n_jobs=-1
        )

        cv_rmse = -cross_val_score(
            reg, X_sev, y_sev,
            cv=kf,
            scoring="neg_root_mean_squared_error",
            n_jobs=-1
        ).mean()

        return cv_rmse

    study = optuna.create_study(direction="minimize", sampler=optuna.samplers.TPESampler(seed=random_state))
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

    best_params = study.best_params
    print(f"🏆 best params  →  power={best_params['p']:.3f} | eta={best_params['eta']:.3f} "
          f"| RMSE={study.best_value:,.2f}")

    # ──────────────────────────────────────────────────────────
    # 4.  Fit severity model with best hyper‑parameters on *all* positives
    reg = XGBRegressor(
        objective="reg:tweedie",
        tweedie_variance_power=best_params['p'],
        learning_rate=best_params['eta'],
        max_depth=best_params['max_depth'],
        subsample=best_params['subsample'],
        colsample_bytree=best_params['colsample_bytree'],
        n_estimators=best_params['n_estimators'],
        random_state=random_state,
        eval_metric="rmse",
        n_jobs=-1
    )
    reg.fit(X_sev, y_sev)

    # ──────────────────────────────────────────────────────────
    # 5.  Unified prediction function:  E[Y] = P(Y>0) × E[Y|Y>0]
    def predict(x_new):
        p_claim  = clf.predict_proba(x_new)[:, 1]
        mean_sev = reg.predict(x_new)
        return p_claim * mean_sev

    return {
        "clf": clf,
        "reg": reg,
        "study": study,
        "predict": predict
    }

# X_train, y_train  →  your 37 000‑row training set (features & LC / HALC)

zit_model = train_zit_xgb(X_lc, y_lc, n_trials=60)

# get predictions
y_hat = zit_model["predict"](X_lc)

# quick RMSE on training (sanity check)
rmse = np.sqrt(mean_squared_error(y_lc, y_hat))
print("train RMSE:", rmse)

# Optuna history / best params
print(zit_model["study"].best_params)

# prompt: calculate the model accuracy based on actual lc values vs the predicted

import numpy as np
# Assuming y_lc contains the actual loss costs and y_hat contains the predicted loss costs
# Calculate the accuracy based on a threshold (e.g., 5% difference)
threshold = 0.05  # 5% difference

# Calculate the absolute difference between actual and predicted values
diff = np.abs(y_lc - y_hat)

# Calculate the percentage difference
percentage_diff = diff / y_lc  # Handle potential division by zero

# Count the number of predictions within the threshold
correct_predictions = np.sum(percentage_diff <= threshold)

# Calculate the accuracy
accuracy = correct_predictions / len(y_lc)

print(f"Accuracy (within {threshold * 100:.0f}% difference): {accuracy * 100:.2f}%")


#Alternative accuracy metrics (example using R^2)

from sklearn.metrics import r2_score

r_squared = r2_score(y_lc, y_hat)

print(f"R-squared: {r_squared}")

# prompt: plot the actual and predicted lc values

import matplotlib.pyplot as plt

# Assuming y_lc contains the actual loss costs and y_hat contains the predicted loss costs
plt.figure(figsize=(10, 6))
plt.plot(y_lc.values, label='Actual Loss Cost')
plt.plot(y_hat, label='Predicted Loss Cost')
plt.xlabel('Data Point Index')
plt.ylabel('Loss Cost')
plt.title('Actual vs Predicted Loss Costs')
plt.legend()
plt.grid(True)
plt.show()

# X_train, y_train  →  your 37 000‑row training set (features & LC / HALC)

zit_model_halc = train_zit_xgb(X_halc, y_halc, n_trials=60)

# get predictions
y_hat_halc = zit_model["predict"](X_halc)

# quick RMSE on training (sanity check)
rmse = np.sqrt(mean_squared_error(y_halc, y_hat_halc))
print("train RMSE:", rmse)

# Optuna history / best params
print(zit_model_halc["study"].best_params)

# prompt: calculate the model accuracy based on actual halc values vs the predicted

import matplotlib.pyplot as plt
import numpy as np
# Assuming y_halc contains the actual historically adjusted loss costs and y_hat_halc contains the predicted historically adjusted loss costs
# Calculate the accuracy based on a threshold (e.g., 5% difference)
threshold = 0.05  # 5% difference

# Calculate the absolute difference between actual and predicted values
diff_halc = np.abs(y_halc - y_hat_halc)

# Calculate the percentage difference
percentage_diff_halc = diff_halc / y_halc  # Handle potential division by zero

# Count the number of predictions within the threshold
correct_predictions_halc = np.sum(percentage_diff_halc <= threshold)

# Calculate the accuracy
accuracy_halc = correct_predictions_halc / len(y_halc)

print(f"Accuracy for Historically Adjusted Loss Cost (within {threshold * 100:.0f}% difference): {accuracy_halc * 100:.2f}%")


#Alternative accuracy metrics (example using R^2) for HALC

r_squared_halc = r2_score(y_halc, y_hat_halc)

print(f"R-squared for Historically Adjusted Loss Cost: {r_squared_halc}")


#Plot for HALC

# Assuming y_halc contains the actual loss costs and y_hat_halc contains the predicted loss costs
plt.figure(figsize=(10, 6))
plt.plot(y_halc.values, label='Actual Historically Adjusted Loss Cost')
plt.plot(y_hat_halc, label='Predicted Historically Adjusted Loss Cost')
plt.xlabel('Data Point Index')
plt.ylabel('Historically Adjusted Loss Cost')
plt.title('Actual vs Predicted Historically Adjusted Loss Costs')
plt.legend()
plt.grid(True)
plt.show()

# prompt: calculate the tweedie deviance for lac and halc both

import numpy as np
from sklearn.metrics import mean_tweedie_deviance

# ... (your existing code)

# Calculate Tweedie deviance for Loss_Cost (y_lc)
tweedie_deviance_lc = mean_tweedie_deviance(y_lc, y_hat, power=zit_model["study"].best_params['p'])
print(f"Tweedie Deviance for Loss_Cost: {tweedie_deviance_lc}")


# Calculate Tweedie deviance for Historically_Adjusted_Loss_Cost (y_halc)
tweedie_deviance_halc = mean_tweedie_deviance(y_halc, y_hat_halc, power=zit_model_halc["study"].best_params['p'])
print(f"Tweedie Deviance for Historically_Adjusted_Loss_Cost: {tweedie_deviance_halc}")

# prompt: i want to see the df containing actual and predicted values of lc and halc side by side where actuals are greater than 0

import pandas as pd
# Create a DataFrame with actual and predicted values
comparison_df = pd.DataFrame({'Actual_LC': y_lc, 'Predicted_LC': y_hat, 'Actual_HALC': y_halc, 'Predicted_HALC': y_hat_halc})

# Filter the DataFrame to include only rows where actual LC is greater than 0
comparison_df = comparison_df[comparison_df['Actual_LC'] > 0]

# Display the DataFrame
comparison_df

# prompt: why are actual and predicted values for actuals>0 so far off

import matplotlib.pyplot as plt
# Analyze the differences between actual and predicted values where actuals > 0
comparison_df['Difference_LC'] = comparison_df['Actual_LC'] - comparison_df['Predicted_LC']
comparison_df['Percentage_Difference_LC'] = (comparison_df['Difference_LC'] / comparison_df['Actual_LC']) * 100

#Analyze the differences between actual and predicted values where actuals > 0 for HALC
comparison_df['Difference_HALC'] = comparison_df['Actual_HALC'] - comparison_df['Predicted_HALC']
comparison_df['Percentage_Difference_HALC'] = (comparison_df['Difference_HALC'] / comparison_df['Actual_HALC']) * 100

# Display descriptive statistics for the differences
print(comparison_df[['Difference_LC', 'Percentage_Difference_LC','Difference_HALC', 'Percentage_Difference_HALC']].describe())


# Identify potential outliers or patterns in the differences
print(comparison_df.sort_values('Percentage_Difference_LC', ascending=False).head(20))
print(comparison_df.sort_values('Percentage_Difference_HALC', ascending=False).head(20))

# Plot the actual vs predicted values for a closer look at the deviations
plt.figure(figsize=(10, 6))
plt.scatter(comparison_df['Actual_LC'], comparison_df['Predicted_LC'], alpha=0.5)
plt.xlabel('Actual Loss Cost')
plt.ylabel('Predicted Loss Cost')
plt.title('Actual vs Predicted Loss Costs (Actuals > 0)')
plt.plot([0, max(comparison_df['Actual_LC'])], [0, max(comparison_df['Actual_LC'])], color='red', linestyle='--') # Add a diagonal line for reference
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(comparison_df['Actual_HALC'], comparison_df['Predicted_HALC'], alpha=0.5)
plt.xlabel('Actual Historically Adjusted Loss Cost')
plt.ylabel('Predicted Historically Adjusted Loss Cost')
plt.title('Actual vs Predicted Historically Adjusted Loss Costs (Actuals > 0)')
plt.plot([0, max(comparison_df['Actual_HALC'])], [0, max(comparison_df['Actual_HALC'])], color='red', linestyle='--') # Add a diagonal line for reference
plt.grid(True)
plt.show()

#Investigate feature distributions for large errors:
#For example, examine the distributions of 'Premium_Amt_Current_Yr', 'Customer_Loyalty', etc., separately for data points where the percentage difference is very high and very low.  This will help identify if specific feature values correlate with prediction errors.


#Check for potential data issues:
#Examine rows where the percentage difference is significantly high for both loss cost and historically adjusted loss cost. Check if any data entry errors, inconsistent values, missing information, or other oddities might be the cause.

#Re-evaluate Feature Engineering
#Consider adding interaction terms, polynomial features, or other transformations to the features.
#Consider removing or adjusting features that show high correlations with large errors.
#Experiment with different feature selection techniques.


#Tune hyperparameters more carefully
#The Tweedie power parameter and other XGBoost hyperparameters are crucial for a Tweedie model.
#Retune your model after addressing the identified issues.
#Try a wider range of values for hyperparameters, or use a more sophisticated tuning method.

import numpy as np, optuna, pandas as pd
from sklearn.model_selection import StratifiedKFold, cross_val_score
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, make_scorer

def train_status_xgb(X, y,
                     n_trials=40,
                     random_state=42):
    """
    Tune an XGBClassifier (binary:logistic) for highly‑imbalanced claim‑status data.
    Returns a fitted model and the Optuna study.
    """
    # ── keep rows with non‑missing status
    mask = ~pd.isna(y)
    Xc, yc = X[mask], y[mask].astype(int)

    pos_wt = (len(yc) - yc.sum()) / yc.sum()      # imbalance ratio
    skf = StratifiedKFold(n_splits=5,
                          shuffle=True,
                          random_state=random_state)

    def objective(trial):
        eta   = trial.suggest_float('eta', 0.01, 0.2, log=True)
        depth = trial.suggest_int('max_depth', 3, 8)
        subs  = trial.suggest_float('subsample', 0.6, 1.0)
        col   = trial.suggest_float('colsample_bytree', 0.6, 1.0)
        n_est = trial.suggest_int('n_estimators', 300, 2000)

        clf = XGBClassifier(
            objective="binary:logistic",
            learning_rate=eta,
            max_depth=depth,
            subsample=subs,
            colsample_bytree=col,
            n_estimators=n_est,
            scale_pos_weight=pos_wt,
            random_state=random_state,
            eval_metric="aucpr",       # AUPRC handles imbalance better than AUROC
            n_jobs=-1
        )

        aucpr = cross_val_score(
            clf, Xc, yc, cv=skf,
            scoring="average_precision",   # AUPRC
            n_jobs=-1
        ).mean()

        return -aucpr                      # Optuna minimises

    study = optuna.create_study(direction="minimize",
                                sampler=optuna.samplers.TPESampler(seed=random_state))
    study.optimize(objective,
                   n_trials=n_trials,
                   show_progress_bar=True)

    print("🏆 best params:", study.best_params,
          "| CV‑AUPRC:", -study.best_value)

    best = XGBClassifier(
        objective="binary:logistic",
        scale_pos_weight=pos_wt,
        eval_metric="aucpr",
        n_jobs=-1,
        random_state=random_state,
        **study.best_params
    )
    best.fit(Xc, yc)
    return best, study

test_df = pd.read_csv('cleaned_test.csv')
df = pd.read_csv('cleaned_data.csv')

drop_cols = [
    'Unnamed: 0', 'ID',
    'Start_Date_Contract', 'Date_Last_Renewal', 'Date_Next_Renewal',
    'Date_Of_Birth', 'Date_Of_DL_Issuance',
    'Loss_Cost', 'Historically_Adjusted_Loss_Cost', 'Claim_Status',
    'Total_Cost_Claims_Current_Yr', 'Total_Number_Claims_Current_Yr',
    'Total_Number_Claims_Entire_Duration', 'Ratio_Claims_Total_Duration_Force'
]

top_features_cs = [
    'Premium_Amt_Current_Yr', 'Customer_Loyalty', 'Power_Wt_Ratio', 'Vehicle_Power_HP',
    'Vehicle_Wt_Kg', 'Car_Age', 'Years_Driving', 'Ratio_Premium_Car_Value',
    'Motorbikes_Vans_Cars_Agricultural', 'Cylinder_Capacity',
    'Policies_Terminated_Non_Payment', 'Car_Age_Cat_Recent', 'Car_Age_Cat_Standard',
    'Max_Policy_Simultaneous_Force', 'Non_Continuation_Insurance_Flag'
]

X_train = df.drop(columns=drop_cols)
X_train = pd.get_dummies(X_train, drop_first=True)
X_train = X_train.reindex(columns=top_features_cs, fill_value=0)
y_train = df['Claim_Status']
X_raw = df.drop(columns=drop_cols + ['Claim_Status'])
y = df['Claim_Status']

test_df = test_df.drop(columns=[
    'Unnamed: 0', 'Start_Date_Contract', 'Date_Last_Renewal', 'Date_Next_Renewal',
    'Date_Of_Birth', 'Date_Of_DL_Issuance'
])

X_test = pd.get_dummies(test_df, drop_first=True)

for col in top_features_cs:
    if col not in X_test.columns:
        X_test[col] = 0
X_test = X_test[top_features_cs]

status_model, status_study = train_status_xgb(X_train, y, n_trials=60)

# Probability a record will have a claim
p_claim = status_model.predict_proba(X_test[top_features_cs])[:, 1]

# (Optional) threshold tuning – e.g. maximise F1 on a validation slice

# X_test already holds the records you want to score
# make sure it has the exact 15 feature columns in top_features_cs
p_claim = status_model.predict_proba(X_test[top_features_cs])[:, 1]

# attach to the test‐set DataFrame if you like
X_test["Predicted_Claim_Prob"] = p_claim

X_test